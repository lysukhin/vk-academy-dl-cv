{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   },
   "source": [
    "<img style=\"float: left;\" src=\"resources/made.jpg\" width=\"35%\" height=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Академия MADE\n",
    "## Семинар 10: Metric learning\n",
    "Эдуард Тянтов, руководитель направления машинного обучения в Почте и Портале Mail.ru\n",
    "\n",
    "В рамках семинар обучим модель распознавания лиц, используя несколько техник Metric Learning, \n",
    "на небольшом подмножестве MSCeleb.\n",
    "Данные можно найти по https://cloud.mail.ru/public/33FM/ucXu3rW7n/ (это небольшая подвыборка из MSCeleb: 50k элементов)\n",
    "Код написан на python 3.7.\n",
    "Для выполнения работы нужны следующие пакеты:\n",
    "* torch, torchvision (conda install pytorch torchvision cudatoolkit=9.2 -c pytorch)\n",
    "* opencv, matplotlib (conda install -c conda-forge opencv matplotlib)\n",
    "* pandas, jupyter (conda install pandas jupyter)\n",
    "* faiss (conda install -c pytorch faiss-cpu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# План\n",
    "* Смотрим на данные\n",
    "* обучаем сеть c Softmax\n",
    "* Center-loss\n",
    "* Arcface\n",
    "* Оцениваем все 3 метода по валидации\n",
    "* Тестируем на faiss/hnsw лучший из них"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import datetime, time\n",
    "from utils import save_object, load_object\n",
    "print(torch.__version__)\n",
    "from IPython.display import Image\n",
    "device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "num_devices = torch.cuda.device_count()\n",
    "print ('num_devices=%d' % num_devices)\n",
    "# set only one device for now\n",
    "if device == 'cuda':\n",
    "   device = 'cuda:0'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data paths\n",
    "# пропишите два путя из распакованного архива с данными\n",
    "data_path = '/home/tyantov/small_celeb/'\n",
    "test_data_path = '/home/tyantov/test_small_celeb/'\n",
    "#\n",
    "pretrained_arc = 'resources/arc_resnet18_512_16_epochs_checkpoint.pth.tar'\n",
    "pretrained_softmax = 'resources/softmax_resnet18_512_10_epochs_checkpoint.pth.tar'\n",
    "pretrained_centerloss = 'resources/centerloss_resnet18_512_10_epochs_checkpoint.pth.tar'\n",
    "lfw_path = 'resources/lfw.bin' # lfw test dataset dump\n",
    "\n",
    "# sanity checks\n",
    "assert os.path.exists(data_path)\n",
    "assert os.path.exists(test_data_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Данные\n",
    "Давайте посмотрим, что из себя представляют данные из MSCeleb.\n",
    "Датасет представляет из себя набор папок, в каждой папке набор вырезанных лиц одного человека.\n",
    "``\n",
    ">$ ls small_celeb | head -5\n",
    "\n",
    "100 \n",
    "1000\n",
    "10000\n",
    "10001\n",
    "10002``\n",
    "``\n",
    ">$ ls small_celeb/100/100\n",
    "\n",
    "2746.jpg  2747.jpg  2748.jpg  2749.jpg  2750.jpg  2751.jpg  2752.jpg  2753.jpg  2754.jpg  2755.jpg  2756.jpg  2757.jpg  2758.jpg  2759.jpg  2760.jpg  2761.jpg  2762.jpg  2763.jpg  2764.jpg``"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте посмотрим как выглядят данные, по персонам."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "persons = os.listdir(data_path)\n",
    "person = persons[np.random.randint(0,len(persons)-1)]\n",
    "person_folder = os.path.join(data_path, person, person)\n",
    "files = os.listdir(person_folder)\n",
    "\n",
    "SAMPLE_SIZE = len(files)\n",
    "NUM_COLS = 5\n",
    "NUM_ROWS = SAMPLE_SIZE // NUM_COLS + int(SAMPLE_SIZE % NUM_COLS != 0)\n",
    "plt.figure(figsize=(20, 2 * NUM_ROWS))\n",
    "\n",
    "for i, _file in enumerate(files):\n",
    "    image = cv2.imread(os.path.join(person_folder, _file))\n",
    "    text = _file\n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i+1)\n",
    "    plt.imshow(image[:, :, ::-1])\n",
    "    plt.title(text)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_classes = len(persons)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Тренировка\n",
    "\n",
    "Сначала создаим датасет. Начнем с аугментаций.\n",
    "Cutout - отличная аугментация, которую вы возможно не знали.\n",
    "Суть простая - зануляем пиксили в рандомно регионе заданного размера, имитируя таким образом\n",
    "occlusion."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Аугментация, которая вырезает кусок заданного размера из картинки\n",
    "    статья: https://arxiv.org/pdf/1708.04552.pdf\"\"\"\n",
    "    def __init__(self, size=8):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        _, h, w = tensor.shape\n",
    "        y = np.random.randint(0, h)\n",
    "        x = np.random.randint(0, w)\n",
    "        tensor[:, y - self.size:y + self.size, x - self.size:x + self.size] = 0\n",
    "\n",
    "        return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выглядит при аугментации это след. образом:\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/cutout.png\" width=\"35%\" height=\"35%\">\n",
    "\n",
    "Зададим остальные трансформации:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# импорт стандартных библиотек для трансформаций и датасета\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "# inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.\n",
    "if device == 'gpu':\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                     std=[0.5, 0.5, 0.5])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # вертикальный флип нам не подойдет, если нужно распознавать такие фотки,\n",
    "    # то на детекторе надо предсказывать перевернутость головы, что может породить ошибки при инференсе\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "    Cutout(size=14) # по сути у нас две аугментации всего\n",
    "    ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), normalize]) # при инференсе только нормализуем\n",
    "\n",
    "# используем стандартный датасет от пайторча\n",
    "#the images are arranged in this way:\n",
    "#         root/dog/xxx.png\n",
    "#         root/dog/xxy.png\n",
    "#         root/dog/xxz.png\n",
    "#\n",
    "#         root/cat/123.png\n",
    "#         root/cat/nsdf3.png\n",
    "#         root/cat/asd932_.png\n",
    "train_dataset = torchvision.datasets.ImageFolder(data_path, train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(test_data_path, test_transform)\n",
    "\n",
    "print('The size of the train dataset:', len(train_dataset))\n",
    "print('The size of the test dataset:', len(test_dataset))\n",
    "\n",
    "# И финально зададим даталоадер\n",
    "\n",
    "batch_size = 160 # ставьте 40 и сеть при обучении займет 3GB, 160 - 10.5Gb\n",
    "train_sampler = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, drop_last=False, sampler=train_sampler)\n",
    "\n",
    "# Обратите внимание, что shuffle & drop_last на инференсе False\n",
    "# На что влиет num_workers ? Как его подбирать ? Как зависит от дисков?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перейдем к различным loss'ам.\n",
    "Сначала обучим сеть с помощью софтмакса\n",
    "Нам надо сделать небольшую модификацию линейного слоя, чтобы он всегда\n",
    "нормализовал веса и входной тензор. Для того, чтобы работать в на единичной сфере.\n",
    "Чем это удобно кстати ?\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/softmax_norm.jpg\" width=\"35%\" height=\"35%\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "class LinearScoring(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearScoring, self).__init__(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, input, target=None):\n",
    "        \"The only difference is normalization of the input and weights\"\n",
    "        input = F.normalize(input, p=2, dim=1)\n",
    "        scores = F.linear(input, F.normalize(self.weight, p=2, dim=1))\n",
    "        return scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Здесь функции для инициализации разных лоссов\n",
    "import torch.nn as nn\n",
    "from models.scoring import ArcFaceScoring\n",
    "from models.center_loss import CenterLoss\n",
    "def init_arcface_scoring(emb_size, num_classes, m, s):\n",
    "    scoring = ArcFaceScoring(m, s, in_features=emb_size, out_features=num_classes, device=device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return scoring, criterion\n",
    "\n",
    "def init_softmax(emb_size, num_classes):\n",
    "    scoring = LinearScoring(emb_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return scoring, criterion\n",
    "\n",
    "def init_centerloss(emb_size, num_classes):\n",
    "    scoring, criterion = init_softmax(emb_size, num_classes)\n",
    "    center_loss = CenterLoss(num_classes, emb_size)\n",
    "    return scoring, criterion, center_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# params\n",
    "center_loss, optimzer4center = None, None\n",
    "cl_loss_weight = 5e-3 # вес при лоссе относительно CE\n",
    "head_size = 7 * 7 * 512 # size of the last resnet layer's output\n",
    "emb_size = 512\n",
    "lr, center_lr = 0.01, 0.1\n",
    "momentum=0.9\n",
    "weight_decay = 0.0005\n",
    "# Init scoring\n",
    "#scoring, criterion = init_arcface_scoring(emb_size, num_classes, m=0.5, s=64.0)\n",
    "scoring, criterion = init_softmax(emb_size, num_classes)\n",
    "#scoring, criterion, center_loss = init_centerloss(emb_size, num_classes)\n",
    "# Создадим модель и оптимизатор\n",
    "\n",
    "from models.resnet import resnet\n",
    "model = resnet(18, num_classes, emb_size=emb_size, scoring=scoring, head_size=head_size)\n",
    "\n",
    "#model = torch.nn.parallel.DataParallel(model.to(device))\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)\n",
    "\n",
    "if center_loss is not None:\n",
    "    center_loss = center_loss.to(device)\n",
    "    optimzer4center = torch.optim.SGD(center_loss.parameters(), lr =center_lr)\n",
    "# TODO change lr over time scheduler\n",
    "# <Изучаем модель в models/resnet.py>\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Валидация\n",
    "Валидироваться будем на стандартном небольшом тесте LFW (http://vis-www.cs.umass.edu/lfw/)\n",
    "У нас есть список пар фотографий и список соответствий is_same_list: lfw.py#L224\n",
    "в issame_list хранится информация о том, являются два лица из пары одним и тем же человеком или нет\n",
    "мы находим эмбединг для лица (прогоняем два раза для лица и отзеркаленного лица и усредням)\n",
    "далее для разных трешхолдов проверяем, что два заданных лица одного и того же человека считаются нашей сеткой одной парой:\n",
    "lfw.py#L157\n",
    "\n",
    "Обученные модели там выбивают примерно 99.7"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][300/314]\tTime 0.384 (0.389)\tData time 0.000 (0.002)\tLoss 14.5499 (14.5000)\t\n",
      "testing lfw..\n",
      "Epoch: [5][100/314]\tTime 0.383 (0.391)\tData time 0.000 (0.005)\tLoss 7.8099 (7.8395)\t\n",
      "Epoch: [5][200/314]\tTime 0.386 (0.389)\tData time 0.000 (0.003)\tLoss 7.8435 (7.8348)\t\n",
      "Epoch: [5][300/314]\tTime 0.386 (0.388)\tData time 0.000 (0.002)\tLoss 7.8124 (7.8288)\t\n",
      "testing lfw..\n",
      "val lfw acc 0.84917+-0.01841 , acc2 0.86167+-0.01751 \n",
      "\n",
      "Previous epoch duration: 0:02:26.654951\n",
      "Current epoch start time: 2020-04-30 14:31:39.317840\n",
      "Epoch: [6][0/314]\tTime 0.893 (0.893)\tData time 0.502 (0.502)\tLoss 7.8100 (7.8100)\t\n",
      "Epoch: [6][100/314]\tTime 0.384 (0.391)\tData time 0.000 (0.005)\tLoss 7.7788 (7.7981)\t\n",
      "Epoch: [6][200/314]\tTime 0.383 (0.388)\tData time 0.000 (0.003)\tLoss 7.7954 (7.7939)\t\n",
      "Epoch: [6][300/314]\tTime 0.393 (0.388)\tData time 0.000 (0.002)\tLoss 7.7823 (7.7888)\t\n",
      "testing lfw..\n",
      "val lfw acc 0.85300+-0.01851 , acc2 0.86600+-0.01592 \n",
      "\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Previous epoch duration: 0:02:26.507101\n",
      "Current epoch start time: 2020-04-30 14:34:07.563375\n",
      "Epoch: [7][0/314]\tTime 0.856 (0.856)\tData time 0.464 (0.464)\tLoss 7.7703 (7.7703)\t\n",
      "Epoch: [7][100/314]\tTime 0.385 (0.390)\tData time 0.000 (0.005)\tLoss 7.7582 (7.7643)\t\n",
      "Epoch: [7][200/314]\tTime 0.383 (0.388)\tData time 0.000 (0.003)\tLoss 7.7425 (7.7564)\t\n",
      "Epoch: [7][300/314]\tTime 0.388 (0.387)\tData time 0.000 (0.002)\tLoss 7.7251 (7.7505)\t\n",
      "testing lfw..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-a0022756e355>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;31m# validate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m     \u001B[0macc1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprec1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_xnorm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membeddings_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'val lfw acc %1.5f+-%1.5f , acc2 %1.5f+-%1.5f \\n'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0macc1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprec1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/made/metric_learning/eval/lfw.py\u001B[0m in \u001B[0;36mtest\u001B[0;34m(lfw_set, model, batch_size, device)\u001B[0m\n\u001B[1;32m    242\u001B[0m                     \u001B[0mnet_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcalc_scores\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 244\u001B[0;31m                 \u001B[0m_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet_out\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    245\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    246\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0membeddings\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-a0022756e355>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;31m# validate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m     \u001B[0macc1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprec1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_xnorm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membeddings_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'val lfw acc %1.5f+-%1.5f , acc2 %1.5f+-%1.5f \\n'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0macc1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprec1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/made/metric_learning/eval/lfw.py\u001B[0m in \u001B[0;36mtest\u001B[0;34m(lfw_set, model, batch_size, device)\u001B[0m\n\u001B[1;32m    242\u001B[0m                     \u001B[0mnet_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcalc_scores\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 244\u001B[0;31m                 \u001B[0m_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet_out\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    245\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    246\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0membeddings\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# validate\n",
    "\n",
    "from eval.loader import load_bin\n",
    "val_set = load_bin(lfw_path, (112, 112))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Traning routine\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "from utils import AverageMeter, save_checkpoint\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, start_epoch, print_freq=100):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    avg_loss = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        emb, scores, losses = model(input, target)\n",
    "        loss = criterion(scores, target)\n",
    "\n",
    "        if center_loss is not None:\n",
    "            cl_loss = center_loss(target, emb)\n",
    "            loss += cl_loss.sum() * cl_loss_weight\n",
    "            optimzer4center.zero_grad()\n",
    "\n",
    "        avg_loss.update(loss.item(), input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if optimzer4center is not None:\n",
    "            optimzer4center.step()\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            line = 'Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                   'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                   'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                epoch, i + start_epoch, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=avg_loss)\n",
    "            print(line)\n",
    "\n",
    "\n",
    "# train\n",
    "from eval.lfw import test\n",
    "start_epoch, epoch_duration, best_prec1 = 0, 0, 0\n",
    "num_epochs = 10\n",
    "\n",
    "checkpoint_path = 'checkpoints/'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"Previous epoch duration: %s\" % epoch_duration)\n",
    "    print(\"Current epoch start time:\", start_time)\n",
    "\n",
    "    train(train_loader, model, criterion, optimizer, epoch, start_epoch)\n",
    "    # validate\n",
    "    acc1, std1, prec1, std2, _xnorm, embeddings_list = test(val_set, model, batch_size, device=device)\n",
    "    print('val lfw acc %1.5f+-%1.5f , acc2 %1.5f+-%1.5f \\n' % (acc1, std1, prec1, std2))\n",
    "\n",
    "    scheduler.step(prec1)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    # save\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': 'resnet18',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, os.path.join(checkpoint_path, 'checkpoint'))\n",
    "\n",
    "print('Training ended, best pre1', best_prec1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте провалидируем модель"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LOAD = True\n",
    "torch.cuda.empty_cache()\n",
    "from utils import load_checkpoint\n",
    "\n",
    "if LOAD:\n",
    "    print('Loading trained for 10 epochs model')\n",
    "    if isinstance(model.scoring, ArcFaceScoring):\n",
    "        load_checkpoint(model, pretrained_arc)\n",
    "        print(open('resources/arcface_train_log.txt', 'r').read())\n",
    "    elif center_loss is not None:\n",
    "        load_checkpoint(model, pretrained_centerloss)\n",
    "        print(open('resources/centerloss_train_log.txt', 'r').read())\n",
    "    else:\n",
    "        load_checkpoint(model, pretrained_softmax)\n",
    "        print(open('resources/softmax_train_log.txt', 'r').read())\n",
    "else:\n",
    "    load_checkpoint(model, os.path.join(checkpoint_path, 'checkpoint_best.pth.tar'))\n",
    "\n",
    "acc1, std1, prec1, std2, _xnorm, embeddings_list = test(val_set, model, batch_size, device=device)\n",
    "print('lfw acc %1.5f+-%1.5f , acc2 %1.5f+-%1.5f' % (acc1, std1, prec1, std2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Напомню разумный максимум 99.7 - годится для замеров на небольшом датасете\n",
    "для больших датасетов скор дорастает до максимального значения и дальше шумит\n",
    "нужно использовать другую более трудную валидацию - например, Megaface"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Center loss\n",
    "Давайте посмотрим на center loss\n",
    "Напомню идею:\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/center_loss_idea.png\" width=\"100%\" height=\"100%\">\n",
    "\n",
    "По сути мы используем два лосса:\n",
    " - дискриминационный (softmax), чтобы обеспечить разделяемость\n",
    " - стягивающий для компактности (center loss)\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/center_loss_arch.png\" width=\"100%\" height=\"100%\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<Смотрим и разбираем код в models/scoring.py>\n",
    "\n",
    "В Backprop вычисляется вот эта формула, хотя строго говоря можно было оставить автограду посчитать ее:\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/center_loss_grad.png\" width=\"40%\" height=\"40%\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arcface\n",
    "\n",
    "Перейдем к Arcface.\n",
    "Напомню основная идея A-Softmax лоссов - оперировать на единичной окружности, где за разделимость\n",
    "отвечает угол между векторами (весами линейного слоя) классов.\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/arcface_idea.png\" width=\"100%\" height=\"100%\">\n",
    "\n",
    "Для этого мы усложняем задачу модели для элементов одного класса, занижая скор по\n",
    "формуле.\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/arcface_reduce.png\" width=\"100%\" height=\"100%\">\n",
    "<img style=\"float: left;\" src=\"resources/arcface_formula.png\" width=\"100%\" height=\"100%\">\n",
    "\n",
    "<Смотрим и разбираем код в models/scoring.py>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search indices\n",
    "Давайте рассмотрим HNSW и Faiss.\n",
    "\n",
    "Для начала давайте посчитаем тестовые embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute embeddings for test set\n",
    "model.eval()\n",
    "\n",
    "img_label_list = test_loader.dataset.imgs # img names & labels\n",
    "embeddings = np.zeros((len(img_label_list), emb_size), dtype=np.float32)\n",
    "targets = np.array([w[1] for w in img_label_list])\n",
    "with torch.no_grad():\n",
    "    for i, (input, target) in enumerate(test_loader):\n",
    "        input = input.to(device)\n",
    "        embed, _, _ = model(input, target=None, calc_scores=False)\n",
    "        embed = embed.to('cpu').numpy() # shape batch_size x emb_size\n",
    "        embeddings[i*batch_size:(i+1)*batch_size,:] = embed\n",
    "print('Test_embeddings are ready', embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Т.к. индексы AKNN предназначены для больших данных (иначе можно перебором все делать), то я прогнал 200k картинок из\n",
    "MSceleb (не из трейна), чтобы на них посмотреть какие у нас получаться метрики и поиграться с параметрами индексов.\n",
    "Использовал для прогона модель arcface, которая загружается выше."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BIG_INDEX_ON = True\n",
    "# load bigger numbers\n",
    "if BIG_INDEX_ON:\n",
    "    embeddings, targets = load_object('resources/msceleb_emb.dump')\n",
    "    img_label_list = load_object('resources/msceleb_names.dump')\n",
    "\n",
    "# exclude M photos of each person\n",
    "import numpy as np\n",
    "M = 1\n",
    "prev, cnt = -1, 0\n",
    "masks = np.zeros((embeddings.shape[0],), dtype=np.bool)\n",
    "for i, v in enumerate(targets):\n",
    "    is_test = (v != prev and cnt < M)\n",
    "    masks[i] = is_test\n",
    "    if is_test:\n",
    "        cnt += 1\n",
    "    else:\n",
    "        prev, cnt = v, 0\n",
    "train_embeddings, train_targets = embeddings[np.invert(masks)], targets[np.invert(masks)]\n",
    "test_embeddings, test_targets = embeddings[masks], targets[masks]\n",
    "print('Shape of embeddings total/train/test', embeddings.shape, train_embeddings.shape, test_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Faiss\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/faiss_ivf.png\" width=\"100%\" height=\"100%\">\n",
    "Два основных параметра, которыми мы рулим:\n",
    "\n",
    "- кол-во центройдов. Определяет на сколько частей мы бьем наше пространство\n",
    "* если их мало, то будет долгий поиск, т.к. почти фулл скан нужно сделать\n",
    "- чем больше центройдов, тем дольше строить индекс\n",
    "- nprobes - кол-во проб (т.е. поисков по центройдам), при =1 берем только самый ближайший, но тогда будет низкая точность AKNN\n",
    " * линейно зависит время поиска"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Будем мерить rank1 и засекать время\n",
    "def evaluate(_index, test_embeddings):\n",
    "    start = time.time()\n",
    "    distances, items = _index.search(test_embeddings, 1)\n",
    "    print('Timing', time.time() - start)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(items.shape[0]):\n",
    "        closest_idx, closest_dist = items[i, 0], distances[i, 0]\n",
    "        scores.append(int(test_targets[i] == train_targets[closest_idx]))\n",
    "    rank1 = sum(scores)/items.shape[0]\n",
    "    print ('Rank1', rank1)\n",
    "    return rank1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import faiss\n",
    "from utils import save_object, load_object\n",
    "\n",
    "num_centroids = 32 # 32 is enough, the more - more expensive\n",
    "faiss_algo = 'IVF%d,Flat' % (num_centroids,) # inverted files with so many centroids\n",
    "print('Сreate faiss index')\n",
    "index = faiss.index_factory(emb_size, faiss_algo)\n",
    "index.verbose = True\n",
    "\n",
    "\n",
    "# Training: по сути треним центройды и Product Quantizer\n",
    "print('train', train_embeddings.shape, train_embeddings.dtype)\n",
    "assert not index.is_trained\n",
    "start = time.time()\n",
    "index.train(train_embeddings)\n",
    "print('Construction done', time.time() - start)\n",
    "\n",
    "# А теперь загружаем данные в индекс, если будем добавлять новые данные, не факт,\n",
    "# что обученные параметры будут эффективно искать по ним\n",
    "#  Но если распределение не меняется то перетренировывать его не надо.\n",
    "print('Add data to index')\n",
    "index.add(train_embeddings)\n",
    "ps = faiss.ParameterSpace()\n",
    "ps.initialize(index)\n",
    "index.search_type = faiss.IndexPQ.ST_PQ # устанавливаем тип индекса с квантизацией\n",
    "print('Ready to query')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сначала запустим фуллскан, чтобы понять какой rank1 максимальный"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# так как это обычный фуллскан, то \"тренировки\" нет\n",
    "index_f = faiss.IndexFlatL2(emb_size)\n",
    "index_f.add(train_embeddings)\n",
    "evaluate(index_f, test_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index.nprobe = 5 # 1 - bad, 5 - ok\n",
    "print('IVF, Num_centroids', num_centroids, 'probes', index.nprobe)\n",
    "evaluate(index, test_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HNSW\n",
    "<img style=\"float: left;\" src=\"resources/hnsw.png\" width=\"50%\" height=\"50%\">\n",
    "Напомню, что алгоритм графовый, иерархичный. Основан на тесном мире.\n",
    "Что важно:\n",
    "\n",
    "- при построении индекса нужно чтобы элементы сэмплировались случайно.\n",
    " * Самые длинные ребра образуются на первых итерациях. Они позволяют быстро перемещаться по графу. Если семлировать из кластеров то длинных ребер не будет.\n",
    "- построение: векторное представление добавляем в граф, чем больше efConstruction (длина списка), тем точнее вставляем в граф\n",
    " * из этого списка мы ищем M лучших соседей и проводим M ребер\n",
    "- efSearch - Кол-во элементов в списке минимальных обнаруженных дистанций.\n",
    "  В нем хранятся упорядоченные дистанции. Наибольшие дистанции вытесняются.\n",
    "  В тот момент когда список не обновился мы считаем что нашли ближайшего соседа\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_h = faiss.IndexHNSWFlat(emb_size, 32)\n",
    "# длина списка вовремя построения\n",
    "index_h.hnsw.efConstruction = 32 # 40 - is enough\n",
    "#\n",
    "start = time.time()\n",
    "index_h.verbose = True\n",
    "index_h.add(train_embeddings)\n",
    "print('Construction HNSW done', time.time() - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# длина списка вовремя поиска\n",
    "index_h.hnsw.efSearch = 8 # 32 almost\n",
    "print('HNSW, efConstruction=%d, efSearch=%d' % (index_h.hnsw.efConstruction, index_h.hnsw.efSearch))\n",
    "evaluate(index_h, test_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сравнение на миллионе\n",
    "https://github.com/facebookresearch/faiss/wiki/Indexing-1M-vectors\n",
    "<img style=\"float: left;\" src=\"resources/index_comp.png\" width=\"100%\" height=\"100%\">\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}