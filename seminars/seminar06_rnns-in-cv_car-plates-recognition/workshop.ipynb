{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADE](resources/made.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия MADE\n",
    "\n",
    "## Курс \"Компьютерное зрение\"\n",
    "\n",
    "\n",
    "\n",
    "### Семинар 6: распознавание автомобильных номеров с помощью CRNN\n",
    "\n",
    "\n",
    "Сегодня мы займемся задачей распознавания автомобильных номеров. Очевидно, она является частным случаем более широкой задачи - распознавания текста на изображениях.\n",
    "\n",
    "Мы не будем затрагивать тему локализации номеров на изображении и сосредоточимся на \"чтении\" текста номерных знаков на уже подготовленных кропах.\n",
    "\n",
    "#### План:\n",
    "1. Общая схема\n",
    "2. Подготовка данных\n",
    "3. Построение модели `CRNN` (`Convolutional Recurrent Neural Network`)\n",
    "4. Интерфейс и применение функции потерь `CTC Loss` (`Connectionist Temporal Classification`)\n",
    "5. Обучение и результаты\n",
    "6. Анализ проблем и что делать дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Общая схема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![box](resources/black_box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу распознавания текста на изображении можно рассматривать под разными углами:\n",
    "- Как классификацию (1 слово = 1 класс);\n",
    "- Как классификацию отдельных символов (то есть сначала разрезать на символы, затем каждый отдельно \"прочитать\");\n",
    "- Как предсказание последовательностей (фиксированной или переменной длины).\n",
    "\n",
    "Первый подход очевидно очень плох.\n",
    "* **TODO: Почему?**\n",
    "\n",
    "Второй подход тоже не лишен недостатков. \n",
    "* **TODO: Каких?** \n",
    "\n",
    "Третий подход не требует специальной подготовки данных и прост с точки зрения архитектуры. К нему мы и обратимся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Recurrent Neural Network (CRNN)\n",
    "\n",
    "[paper](https://arxiv.org/abs/1507.05717)\n",
    "\n",
    "![crnn](resources/crnn.png)\n",
    "\n",
    "Модель CRNN (не путайте с R-CNN - семейством детекторов) предназначена для перевода изображений в текстовый вид. Она устроена следующим образом (см.изображение выше):\n",
    "1. Первый этап - извлечение признаков. Входное изображение пропускается через последовательность сверточных слоев (с активациями, пулингом, вот эти вот всем), в результате чего получается тензор признаков размера, например, `C x H x W`. Если рассматривать в отдельности каждый из срезов вдоль оси \"ширины\" (`W`, последняя размерность), то можно сказать, что у нас получилась последовательность длины `W`. Каждый из этих элементов соответствует определенной области на исходном изображении: ![receptive](resources/receptive.png)\n",
    "2. Второй этап - предсказание вероятностей каждого из символов алфавита **для каждого элемента последовательности**. Полученная на первом шаге последовательность признаков пропускается через рекуррентную сеть, в результате чего на выходе образуется последовательность той же длины, что и на входе. Размер каждого элемента равен мощности алфавита + 1 (этот \"+1\" - символ `blank` для использования `CTC-Loss`).\n",
    "3. Третий этап - либо декодирование (\"транскрипция\") полученных на предыдущем шаге распределений в итоговый текст (при инференсе), либо вычисление `CTC-Loss` между полученными распределениями и `ground-truth`-последовательностями.\n",
    "\n",
    "Быстрый FAQ:\n",
    "* Q: Длина предсказанных последовательностей после первого шага фиксирована?\n",
    "  \n",
    "  A: Да, если используются батчи с изображениями одинакового размера.\n",
    "\n",
    "* Q: Как из \"двумерных\" элементов после первого шага получаются \"столбики\" (как на картинке выше)?\n",
    "  \n",
    "  A: Можно сделать разворачивание в столбец (размер элементов станет -> `C * H`), пулинг по высоте (-> `C`), линейный слой, <ваш вариант>, ...\n",
    "\n",
    "* Q: Длина предсказанных последовательностй после второго шага равна длине предсказанных в итоге слов?\n",
    "\n",
    "  A: Нет, ее следует сделать больше. На последнем шаге используется `CTC-Loss`, который умеет \"сжимать\" предсказанные последовательности (в том числе с помощью `blank`-символов).\n",
    "  \n",
    "  \n",
    "* Q: Как работает `CTC-Loss`?\n",
    "\n",
    "  A: Пересмотрите лекцию, читайте [статью](https://www.cs.toronto.edu/~graves/icml_2006.pdf), смотрите [видео](https://www.youtube.com/watch?v=eYIL4TMAeRI), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision import models\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет представляет собой размеченные изображения автомобильных номеров РФ, собранных из интернета либо сгенерированных автоматически.\n",
    "Формат данных = папка с изображениями + файл конфигурации, в котором в виде списка хранятся записи о тексте на каждом из изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./seminar09_data/\"  # Change to your path with unzipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(PATH_TO_DATA, \"config.json\")\n",
    "images_path = os.path.join(PATH_TO_DATA, \"images\")\n",
    "assert os.path.isfile(config_path)\n",
    "assert os.path.isdir(images_path)\n",
    "\n",
    "with open(config_path, \"rt\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "config_full_paths = []\n",
    "for item in config:\n",
    "    config_full_paths.append({\"file\": os.path.join(images_path, item[\"file\"]),\n",
    "                              \"text\": item[\"text\"]})\n",
    "config = config_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total items in data:\", len(config))\n",
    "print(\"First 3 items:\")\n",
    "for item in config[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 30\n",
    "NUM_COLS = 5\n",
    "NUM_ROWS = SAMPLE_SIZE // NUM_COLS + int(SAMPLE_SIZE % NUM_COLS != 0)\n",
    "\n",
    "random_idxs = np.random.choice(len(config), size=SAMPLE_SIZE, replace=False)\n",
    "plt.figure(figsize=(20, 2 * NUM_ROWS))\n",
    "for i, idx in enumerate(random_idxs, 1):\n",
    "    item = config[idx]\n",
    "    text = item[\"text\"]\n",
    "    image = cv2.imread(item[\"file\"])\n",
    "    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    plt.imshow(image[:, :, ::-1])\n",
    "    plt.title(text)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в данных встречаются номера разных типов. Для учебных целей отфильтруем примеры и оставим только те, которые относятся к \"стандартным\" гражданским номерным знакам, а именно имеющим вид `LDDDLLDD` или `LDDDLLDDD` (`L` = \"letter\", то есть буква, `D` = \"digit\", цифра).\n",
    "Кроме того, отфильтруем примеры, в которые встречаются символы, не входящие в алфавит регистрационных знаков (см. переменную `abc` ниже).\n",
    "\n",
    "**NB: используются только заглавные буквы латинского алфавита и цифры.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ascii_uppercase)  # may be useful for functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits)  # may be useful for functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **TODO: Реализовать функцию `compute_mask()`, которая бы возвращала маску из символов `L`/`D` для переданной строки `text`.**\n",
    "\n",
    "  *Пригодятся переменные `ascii_uppercase` и `digits`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(text):\n",
    "    \"\"\"Compute letter-digit mask of text, e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text. \n",
    "        \n",
    "    Returns:\n",
    "        String of the same length but with every letter replaced by 'L' and every digit replaced by 'D' \n",
    "        or None if non-letter and non-digit character met in text.\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char in digits:\n",
    "            mask.append(\"D\")\n",
    "        elif char in ascii_uppercase:\n",
    "            mask.append(\"L\")\n",
    "        else:\n",
    "            return None\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return \"\".join(mask)\n",
    "\n",
    "assert compute_mask(\"E506EC152\") == \"LDDDLLDDD\"\n",
    "assert compute_mask(\"E123KX99\") == \"LDDDLLDD\"\n",
    "assert compute_mask(\"P@@@KA@@\") is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **TODO: Реализовать функцию `check_in_alphabet()`, которая возвращает `True`, если все символы из строки `text` содержатся в строке `alphabet`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_alphabet(text, alphabet=abc):\n",
    "    \"\"\"Check if all chars in text come from alphabet.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text.\n",
    "        - alphabet: String of alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        True if all chars in text are from alphabet and False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return True\n",
    "\n",
    "assert check_in_alphabet(\"E506EC152\") is True\n",
    "assert check_in_alphabet(\"A123GG999\") is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем обе реализованные функции для прочистки данных от нежелательных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(config):\n",
    "    \"\"\"Filter config items keeping only ones with correct text.\n",
    "    \n",
    "    Args:\n",
    "        - config: List of dicts, each dict having keys \"file\" and \"text\".\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list (config subset).\n",
    "    \"\"\"\n",
    "    config_filtered = []\n",
    "    for item in tqdm.tqdm(config):\n",
    "        text = item[\"text\"]\n",
    "        mask = compute_mask(text)\n",
    "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
    "            config_filtered.append({\"file\": item[\"file\"],\n",
    "                                    \"text\": item[\"text\"]})\n",
    "    return config_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = filter_data(config)\n",
    "print(\"Total items in data after filtering:\", len(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что осталось после фильтрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 30\n",
    "NUM_COLS = 5\n",
    "NUM_ROWS = SAMPLE_SIZE // NUM_COLS + int(SAMPLE_SIZE % NUM_COLS != 0)\n",
    "\n",
    "random_idxs = np.random.choice(len(config), size=SAMPLE_SIZE, replace=False)\n",
    "plt.figure(figsize=(20, 2 * NUM_ROWS))\n",
    "for i, idx in enumerate(random_idxs, 1):\n",
    "    item = config[idx]\n",
    "    text = item[\"text\"]\n",
    "    image = cv2.imread(item[\"file\"])\n",
    "    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    plt.imshow(image[:, :, ::-1])\n",
    "    plt.title(text)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к созданию класса датасета. Что важно:\n",
    "- Конструктор принимает список словарей с ключами `file` и `text` (`config`), строку с алфавитом для предсказания (`alphabet`) и трансформации (`transforms`)\n",
    "- Для обучения нам потребуется возвращать в методе `__getitem__`:\n",
    "  - Изображение номера (фиксированного размера `HxWx3`).\n",
    "  - Текст номера в виде числовой последовательности (т.е. в закодированном виде)\n",
    "  - Длину этой последовательности (требование для обучения с `CTC Loss`)\n",
    "  - Текст в виде строки (для удобства)\n",
    "  \n",
    "  Удобно сложить все эти переменные в словарь и доставать их оттуда по ключам при необходимости `->` `transforms` должны работать со словарем!\n",
    "- Отображение \"текст `<->` числовая последовательность\" будем делать простым индексированием по строке алфавита. Число \"0\" зарезервируем для символа `blank`. \n",
    "  - Например, пусть наш алфавит = `XYZ`. Тогда текст `XXZY` будет представлена как `[1,1,3,2]` (без `blank` было бы `[0,0,2,1]`).\n",
    "  \n",
    "  \n",
    "* **TODO: Реализовать метод `text2seq` для кодирования текстовой последовательности в числовой вид, как сказано выше**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, config, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        \n",
    "        Args:\n",
    "            - config: List of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "            - alphabet: String of chars required for predicting.\n",
    "            - transforms: Transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.alphabet = alphabet\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        for item in self.config:\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Returns dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_names[item]).astype(np.float32) / 255.\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        \n",
    "        Args:\n",
    "            - String of text.\n",
    "            \n",
    "        Returns:\n",
    "            List of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве трансформации будем использовать только ресайз к фиксированному размеру `(320, 64)`. \n",
    "\n",
    "* **TODO: Реализовать для класса `Resize` метод `__call__()`, который бы доставал из словаря `item` изображение, ресайзил к фиксированному размеру и клал обратно в словарь.**   \n",
    "\n",
    "  *Не забудьте про интерполяцию ([stackoverflow](https://stackoverflow.com/questions/3112364/how-do-i-choose-an-image-interpolation-method-emgu-opencv)).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Apply resizing.\n",
    "        \n",
    "        Args: \n",
    "            - item: Dict with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        \n",
    "        Returns: \n",
    "            Dict with image resized to self.size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем датасет вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Resize(size=(320, 64))\n",
    "dataset = RecognitionDataset(config, alphabet=abc, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image shape:\", x[\"image\"].shape)\n",
    "print(\"Seq:\", x[\"seq\"], \"Seq_len:\", x[\"seq_len\"])\n",
    "print(\"Text:\", x[\"text\"])\n",
    "plt.imshow(x[\"image\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст номеров может иметь длину 8 (`LDDDLLDD`) или 9 (`LDDDLLDDD`), а класс `DataLoader` плохо справляется (из коробки) с данными переменного размера в одном батче. Однако, как увидим далее, реализация `ctc_loss` позволяет передавать батч последовательностей в \"склеенном\" виде, поскольку отдельным параметром передаются длины всех последовательностей в батче.\n",
    "\n",
    "Таким образом, если в батче были последовательности длин `[8, 9, 9, 9]`, то мы склеим их в одну последовательность длины `28`, а `ctc_loss` под капотом сам \"нарежет\" ее на части нужной длины.\n",
    "\n",
    "Для этого вручную реализуем функцию `collate_fn`, чтобы `DataLoader` понял, как формировать батчи.\n",
    "\n",
    "*Здесь же реализовано преобразование `image` из формата `np.ndarray` в формат `torch.Tensor`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    \n",
    "    Args:\n",
    "        - batch: List of dataset __getitem__ return values (dicts).\n",
    "        \n",
    "    Returns:\n",
    "        Dict with same keys but values are either torch.Tensors of batched images or sequences or so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for item in batch:\n",
    "        images.append(torch.from_numpy(item[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(item[\"seq\"])\n",
    "        seq_lens.append(item[\"seq_len\"])\n",
    "        texts.append(item[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [dataset[i] for i in range(4)]\n",
    "batch = collate_fn(xs)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image:\", batch[\"image\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"])\n",
    "print(\"Seq_len:\", batch[\"seq_len\"])\n",
    "print(\"Text:\", batch[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Построение модели CRNN-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к построению класса для модели нейросети.\n",
    "Следуя описанию в начале этой страницы, вынесем (1) и (2) этапы пайплайна в отдельные компоненты модели (`self.cnn` и `self.rnn` соответственно).\n",
    "Их можно реализовать практически независимо друг от друга, поэтому после занятия вы сможете самостоятельно поэкспериментировать со своими вариантами архитектур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с построения `feature_extractor`. \n",
    "\n",
    "Для этого возьмем предобученную модель `ResNet18`, отрежем от нее последние два слоя (это `AdaptiveAvgPool2d` и `Linear`), оставив полносверточную часть. После всех сверточных слоев размер входного изображения уменьшается в 32 раза, а значит, входная картинка размером `64x320` превратится в тензор с высотой и шириной `2x10`. Для него мы сделаем пулинг по высоте (смотри параметры `self.pool`), превратив его в тензор размера `1x10`. \n",
    "\n",
    "Получим, что длина последовательности для подачи в `RNN` составляет всего лишь 10 (этого может быть мало для хорошей работы `CTC-Loss` с таргетами длины 8 или 9). Используем трюк (в методе `apply_projection`), применив сверточный слой с ядром (1x1) вдоль размерности ширины (а не глубины, как обычно), увеличив длину последовательности с 10 до `output_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet18')(pretrained=True)\n",
    "        self.cnn = Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor of features (shaped B x C x H x W).\n",
    "            \n",
    "        Returns:\n",
    "            New tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        return x\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = feature_extractor(x)\n",
    "assert y.size() == (1, 1, 512, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, `FeaturesExtractor`:\n",
    "- На вход получает тензор изображения размером `Bx3xHxW`\n",
    "- На выходе отдает тензор признаков размером `Bx1xFxL`, где \n",
    "  - `F` - размерность вектора-токена (определяется числом признаков последнего сверточного слоя `ResNet18`, т.е. 512), \n",
    "  - `L` - \"длина последовательности\" токенов (`self.output_len`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем класс для рекуррентной части пайплайна.\n",
    "\n",
    "Будем использовать `GRU` (`bidirectional` или нет - выбор по параметру). Результат предсказаний после `GRU` дополнительно проведем через линейный слой для формирования итоговой матрицы с `logits`. \n",
    "\n",
    "* **TODO: Реализовать метод `_init_hidden()` для инициализации скрытого состояния `GRU`.**\n",
    "\n",
    "  *Про размерность `hidden_state` можно [посмотреть в документации](https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU).*\n",
    "\n",
    "\n",
    "\n",
    "`GRU` по умолчанию ожидает на вход тензор размера `L x B x F`, где `L` - длина последовательности, `B` - размер батча, `F` - размер одного элемента последовательности. \n",
    "\n",
    "* **TODO: Реализовать приведение тензора из `FeatureExtractor` к необходимому виду в функции `_reshape_features`.**\n",
    "\n",
    "  *Пригодятся методы `torch.Tensor.squeeze()` и `torch.Tensor.permute()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        \n",
    "        Args:\n",
    "            - batch_size: Int size of batch\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def _reshape_features(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor x shaped (B x (C=1) x H x W).\n",
    "        \n",
    "        Returns:\n",
    "            New tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._reshape_features(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_predictor = SequencePredictor(input_size=512, \n",
    "                                       hidden_size=128, \n",
    "                                       num_layers=2, \n",
    "                                       num_classes=len(abc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 512, 20)\n",
    "assert sequence_predictor._reshape_features(x).size() == (20, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequence_predictor(x)\n",
    "assert y.size() == (20, 1, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соберем две части в один класс CRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    \n",
    "    def __init__(self, alphabet=abc,\n",
    "                 cnn_input_size=(64, 320), cnn_output_len=20,\n",
    "                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n",
    "        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n",
    "                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n",
    "                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n",
    "                                                    bidirectional=rnn_bidirectional)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, добавим также функции для декодирования результата `sequence_predictor` в читаемый вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как все работает на случайном тензоре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = crnn(x)\n",
    "assert y.size() == (20, 1, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(y, abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CTC-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели предсказания последовательностей будем использовать `CTC-Loss`. Класс этой функции потерь уже реализован в `PyTorch`, поэтому нам нужно только понять, как правильно подать в него предсказания и `ground-truth`-метки. Для этого обратимся к [документации](https://pytorch.org/docs/stable/nn.functional.html?highlight=ctc#torch.nn.functional.ctc_loss):\n",
    "\n",
    "![ctc-01](resources/ctc_01.png) \n",
    "\n",
    "![ctc-02](resources/ctc_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На что следует обратить внимание:\n",
    "* Функция ожидает на вход не только пару предсказанных и верных последовательностей, но и информацию о длинах этих последовательностей.\n",
    "* Перед тем, как подавать предсказания в лосс, необходимо применить к ним активацию `softmax` и затем взять логарифм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Обучение и результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше никаких `#YOUR CODE HERE` не будет - если все необходимые функции выше реализованы, то можно запускать ячейки ниже и следить за обучением.\n",
    "\n",
    "Можно выставить константу `ACTUALLY_TRAIN=False`, тогда вместо обучения будут загружены логи предварительно выполненного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUALLY_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель (пока все параметры можно оставить по умолчанию - они подойдут для начала):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "crnn.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделим данные на обучающую и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(config)\n",
    "train_size = int(len(config) * 0.8)\n",
    "config_train = config[:train_size]\n",
    "config_val = config[train_size:]\n",
    "\n",
    "train_dataset = RecognitionDataset(config_train, transforms=Resize())\n",
    "val_dataset = RecognitionDataset(config_val, transforms=Resize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объекты типа `DataLoader` для формирования батчей обучения. Обратите внимание на следующие вещи:\n",
    "* Мы передаем функцию `collate_fn` как параметр конструктора;\n",
    "* Значения параметров `shuffle` и `drop_last` отличаются для случаев обучения и валидации - зачем так сделано?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                            drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение (при параметрах по умолчанию эпоха занимает ~22 секунды на GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.train()\n",
    "if ACTUALLY_TRAIN:\n",
    "    for i, epoch in enumerate(range(num_epochs)):\n",
    "        epoch_losses = []\n",
    "\n",
    "        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "            images = b[\"image\"].to(device)\n",
    "            seqs_gt = b[\"seq\"]\n",
    "            seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "            seqs_pred = crnn(images).cpu()\n",
    "            log_probs = log_softmax(seqs_pred, dim=2)\n",
    "            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        print(i, np.mean(epoch_losses))\n",
    "else:\n",
    "    image_train_log = cv2.imread(\"./resources/train_log.png\")\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    plt.imshow(image_train_log[:, :, ::-1], interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    with open(\"./crnn.pth.tar\", \"rb\") as fp:\n",
    "        state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    crnn.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Провалидируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.eval()\n",
    "val_losses = []\n",
    "for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "    images = b[\"image\"].to(device)\n",
    "    seqs_gt = b[\"seq\"]\n",
    "    seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "    log_probs = log_softmax(seqs_pred, dim=2)\n",
    "    seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "    loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                    targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                    input_lengths=seq_lens_pred,  # N\n",
    "                    target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "    val_losses.append(loss.item())\n",
    "\n",
    "print(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем несколько картинок из валидации и посмотрим на предсказанные для них распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ticks = [\"-\"] + [x for x in abc]\n",
    "\n",
    "images = b[\"image\"]\n",
    "seqs_gt = b[\"seq\"]\n",
    "seq_lens_gt = b[\"seq_len\"]\n",
    "texts = b[\"text\"]\n",
    "\n",
    "preds = crnn(images.to(device)).cpu().detach()\n",
    "texts_pred = decode(preds, crnn.alphabet)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    pred_i = preds[:, i, :].T\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    image = images[i].permute(1, 2, 0).numpy()\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(texts[i])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.yticks(range(pred_i.size(0)), y_ticks)\n",
    "    plt.imshow(pred_i)\n",
    "    plt.title(texts_pred[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Что делать дальше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- При обучении и валидации мы смотрели только на значение функции потерь. Обычно принято для оценки качества моделей использовать какую-либо репрезентативную метрику. Реализуйте подсчет `Accuracy` (доля верно предсказанных номеров) на обучающей и валидационной выборке.\n",
    "- Как можно видеть по последним картинкам, наша модель почти ничего не предсказывает в середине последовательности (предсказывает `blank`). Попробуйте изменить архитктуру (или поиграйтесь с параметрами текущей) модели, чтобы более эффективно использовать ее.\n",
    "- Проанализируйте, на каких примерах валидационной (и обучающей) выборки модель ошибается? Что можно сделать, чтобы полечить эти проблемы?\n",
    "- Мы не умеем оценивать, насколько адекватны предсказания модели. Подумайте, как можно оценить \"уверенность\" модели в собственных предсказаниях?\n",
    "- Вспомните, что мы оставили в выборке только \"стандартные\" номера. Попробуйте вернуть в обучение более сложные классы и обучить модель. Что получится?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
