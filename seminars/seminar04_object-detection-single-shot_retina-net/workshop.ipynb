{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADE](images/made.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия MADE\n",
    "\n",
    "## Курс \"Компьютерное зрение\"\n",
    "\n",
    "### Семинар 4: Реализация Single-shot Object Detection на примере RetinaNet\n",
    "\n",
    "Сегодня мы рассмотрим реализацию архитектуры RetinaNet ([arxiv]()) для детектирования объектов за один проход.\n",
    "\n",
    "#### Материалы для чтения:\n",
    "1. Оригинальная статья про RetinaNet: [Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)\n",
    "2. Статья о подходе FPN: [Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "3. Источник кода для семинара: [github/yhenon/pytorch-retinanet](https://github.com/yhenon/pytorch-retinanet)\n",
    "4. Пост [RetinaNet Explained and Demystified](https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/)\n",
    "\n",
    "#### План\n",
    "1. (Recap) Подходы к детектированию\n",
    "2. Реализация компонентов RetinaNet\n",
    "3. Дообучение RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (Recap) Подходы к детектированию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Детектирование на основе регионов (region-based): \n",
    "  * Отдельная голова для предсказания возможных областей с объектами (Region Proposal Network, RPN)\n",
    "  * Отдельные головы для классификации и уточнения границ регионов \n",
    "  * Все головы работают поверх признаков, извлеченных \"базовой\" моделью (она же *backbone*, она же *ствол* и т.д.)\n",
    "  * Полученные прямоугольники объектов фильтруются с помощью процедуры *Non-maxima suppression (NMS)*\n",
    "  * Пример: [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf) (из семейства R-CNN):\n",
    "      * Признаки вычисляются один раз и являются \"общими\" для всех ветвей\n",
    "      * Голова RPN получает на вход кропы, заданные набором \"якорных\" прямоугольников (они же *anchor boxes*, *default boxes*, *proposals*, ...)\n",
    "      * Для каждого кропа предсказывается вероятность содержания в нем объекта и уточняющие координаты границ\n",
    "      * Отобранные якорные прямоугольники поступают на вход ветвей классификации и регрессии\n",
    "  \n",
    "      ![faster_rcnn](images/faster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Детектирование \"в один проход\" (single-shot):\n",
    "  * Идея с якорными прямоугольниками остается\n",
    "  * Фактически, перенесли головы для классификации и регресии сразу внутрь RPN\n",
    "  * Предсказания классов и уточняющих координат производится сразу для всех якорей\n",
    "  * Пример: [Single Shot Multibox Detector](https://arxiv.org/pdf/1512.02325.pdf)\n",
    "    * Предсказания собираются с разных глубин базовой сети\n",
    "    \n",
    "    ![ssd](images/ssd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевые особенности RetinaNet:\n",
    "* Использование специальной функции потерь (*Focal Loss*) \n",
    "  * для борьбы с дисбалансом классов при обучении (обычно \"объектов\" на изображении меньше, чем \"фона\")\n",
    "* Комбинирование признаков с разных глубин сети  (*Features Pyramid*) \n",
    "  * для \"уравнивания\" семантической силы нейронов на разных масштабах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Реализация компонентов RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device is:\", device)\n",
    "\n",
    "\n",
    "from visualization import show_image, draw_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим тестовое изображение и сделаем из него батч размера 1. Это пригодится нам для демонстрации работы отдельных компонентов модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(image, device=None):\n",
    "    tensor = to_tensor(image).unsqueeze(0)\n",
    "    tensor = normalize(tensor,\n",
    "                       mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225))\n",
    "    if device:\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_example = cv2.imread(\"images/zoo.jpg\")\n",
    "\n",
    "\n",
    "image_example = cv2.cvtColor(image_example, cv2.COLOR_BGR2RGB)\n",
    "print(\"Image shape\", image_example.shape)\n",
    "\n",
    "tensor_example = make_tensor(image_example, device)\n",
    "print(\"Tensor size\", tensor_example.size())\n",
    "\n",
    "show_image(image_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку модель имеет несколько отдельных модулей, реализуем их и положим в один композиционный класс:\n",
    "\n",
    "- `RetinaNet`\n",
    "  - `.backbone` - нейросеть для извлечения признаков; модифицированный `ResNet`, возвращающий карты активаций (вместо логитов);\n",
    "  - `.fpn` - нейросеть для построения пирамиды признаков из выходов `.backbone`;\n",
    "  - `.anchor_generator` - класс для построения наборов \"якорных\" боксов (anchors), на основе которых будет производится поиск объектов;\n",
    "  - `.cls_head` - нейросеть для классификации каждого из anchor boxes;\n",
    "  - `.reg_head` - нейросеть для регрессии уточняющих коэффициентов координат каждого из anchor boxes;\n",
    "  - `.focal_loss` - реализация лосс-функции (да, ее можно поместить прямо в модель)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone: Модифицированный ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet - семейство архитектур, у которого главная вычислительная единица - это т.н. residual-block:\n",
    "\n",
    "![residuals](images/residual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели `ResNet-*` состоят из блоков, объединенных в четыре слоя (`layerX`); между слоями происходит 1) понижение размерности HW и 2) увеличение \"ширины\" блоков (число каналов в картах).\n",
    "\n",
    "**NB: картинка ниже - для сети ResNet-34, в которой используются \"простые\" all-3x3 residual-блоки, как на картинке выше слева. В ResNet-50 используются `bottleneck`-блоки (соответственно сверху справа).\n",
    "Число слоев у этих моделей одинаковое (а вот число блоков в слоях и их ширина отличаются).**\n",
    "\n",
    "![resnet34](images/resnet34.png)\n",
    "\n",
    "Обычно (для задач типа классификации или регрессии) на выходе модели оказывается вектор (размера `C` = числу классов или 1 - для регрессии).\n",
    "Но для задачи детектирования понадобятся промежуточные карты активаций, на основе который будет построена пирамида. \n",
    "На рисунке выше указаны карты `C3`, `C4` и `C5` - их и должен вернуть метод `.forward` нашей базовой сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: реализовать метод `.forward()`, который должен возвращать карты активаций после слоев `layer2`, `layer3`, `layer4` в виде списка/кортежа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from retinanet.utils import BasicBlock, Bottleneck  # See code\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, block, layers_sizes):\n",
    "        \"\"\"\n",
    "        ResNet constructor.\n",
    "        \n",
    "        Args:\n",
    "            - num_classes: int number of outputs for model.\n",
    "            - block: class to use as residual block in model (BasicBlock or Bottleneck).\n",
    "            - layers_sizes: list of sizes (number of blocks) for each of the 4 layers.\n",
    "        \"\"\"\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # Input convolution: larger (7x7) and with increased stride (2).\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Four layers with residual blocks. Each layer has similar structure\n",
    "        self.layer1 = self._make_layer(block, 64, layers_sizes[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers_sizes[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers_sizes[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers_sizes[3], stride=2)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
    "        \"\"\"\n",
    "        Method to create layer for ResNet using residual blocks.\n",
    "        \n",
    "        Args:\n",
    "            - block: class to use as residual block in model (BasicBlock or Bottleneck).\n",
    "            - planes: number of channels in layer's activation maps (also known as layer 'width').\n",
    "            - num_blocks: number of blocks in layer.\n",
    "            \n",
    "        Returns:\n",
    "            nn.Sequential object containing residual blocks.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, num_blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Method for \"__call__\".\n",
    "        \n",
    "        Args:\n",
    "            - inputs: batch of images shaped Bx3xHxW.\n",
    "            \n",
    "        Returns:\n",
    "            List of feature maps tensors from layers 2, 3, 4.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: \n",
    "        # Write code to run all the model layers (do not forget to run input convolution/bn/activation/pooling) \n",
    "        # and to return feature maps from layer2, layer3, layer4.\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "                \n",
    "        ### END OF YOUR CODE\n",
    "        \n",
    "        return (x2, x3, x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 80\n",
    "\n",
    "print(\"ResNet50:\")\n",
    "resnet50_layers_sizes = [3, 4, 6, 3]      \n",
    "resnet50 = ResNet(num_classes=num_classes, block=Bottleneck, layers_sizes=resnet50_layers_sizes).to(device)\n",
    "\n",
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature maps sizes from layer2-4 of ResNet50:\")\n",
    "feature_maps_example = resnet50(tensor_example)\n",
    "for fm in feature_maps_example:\n",
    "    print(fm.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Pyramid: собираем пирамиду активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к модулю, который сделает из последовательности карт активаций пирамиду.\n",
    "\n",
    "Идея `Feature Pyramid` в том, чтобы, скомбинировав карты с разных слоев основной сети (`backbone`), получить карты одновременно \"семантически богатые\" и с большим рецептивным полем.\n",
    "\n",
    "![fpn](images/fpn.png)\n",
    "\n",
    "На рисунке выше изображена пирамида из 3 уровней. Авторы [оригинальной статьи](https://arxiv.org/pdf/1708.02002.pdf) указывают (примечание на стр.4), что в их пирамиде 5 слоев, которые обозначаются от $P_3$ до $P_7$. При этом соответствие слоев пирамиды картам активации такое: $C_3 - P_3$, $C_4 - P_4$, $C_5 - P_5$, а два дополнительных слоя пирамиды $P_6$ и $P_7$ получаются последовательно свертками из $P_5$ (уже без сложения с низкоуровневыми картами)\n",
    "\n",
    "Каждая из карт $C_i$ ($C_5$ - $C_3$):\n",
    "1. Сворачивается с ядром 1х1, получается $C'_i$\n",
    "2. Суммируется с выходной картой выше $C'_{i+1}$ (нет у $C_5$), увеличенной вдвое, и после свертки с ядром 3х3 отправляется на выход ($P_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: реализовать кусок метода `.forward()` для вычисления `P3` (по аналогии с `P5` & `P6`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PyramidFeatures(nn.Module):\n",
    "    \n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        \"\"\"\n",
    "        FPN constructor.\n",
    "        \n",
    "        Args:\n",
    "            - C3_size: num features in C3 map.\n",
    "            - C4_size: num features in C4 map.\n",
    "            - C5_size: num features in C5 map.\n",
    "            - feature_size: num features in output maps.\n",
    "        \"\"\"\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # For P5\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # For P4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # For P3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # For P6 \n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # For P7\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Method for \"__call__\".\n",
    "        \n",
    "        Args:\n",
    "            - inputs: List of C3, C4, C5 activation maps from backbone.\n",
    "            \n",
    "        Returns:\n",
    "            List of pyramid feature maps from P3 to P7.\n",
    "        \"\"\"\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "        \n",
    "        # TODO: \n",
    "        # Write code to compute P3_x output map.\n",
    "        # Almost like P4_x & P5_x, but no need for upsampling itself\n",
    "        # (as we do not have 'lower' map P2)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "\n",
    "        return (P3_x, P4_x, P5_x, P6_x, P7_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_sizes = (512, 1024, 2048)  # ResNet50\n",
    "fpn = PyramidFeatures(*fpn_sizes).to(device)\n",
    "\n",
    "print(\"Feature pyramid sizes:\")\n",
    "features_pyramid_example = fpn(feature_maps_example)\n",
    "for fp in features_pyramid_example:\n",
    "    print(fp.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание:\n",
    "* У первого слоя (который мы называем $P_3$) линейные размеры в 8 = $2^3$ раз меньше, чем у оригинального изображения.\n",
    "* На каждом слое пирамиды линейные размеры уменьшаются вдвое.\n",
    "\n",
    "То есть размеры слоя $P_i$ меньше размеров исходного изображения в $2^i$ раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент у нас есть инструменты для извлечения карт признаков из изображения. \n",
    "\n",
    "Теперь, если бы мы работали в парадигме, например, `Fast R-CNN` (не надо так), мы бы запустили на этих признаках алгоритм `SelectiveSearch`, получили координаты предполагаемых локаций с объектами (в виде обрамляющих прямоугольников), \"вырезали\" их проекции из каждого слоя пирамиды признаков и подали в подсети для определения классов и регрессии относительных координат.\n",
    "\n",
    "Но, поскольку мы работаем в парадигме single-shot-детектирования, мы сделаем все за один проход. \n",
    "С каждым супер-пикселем любого из слоев пирамиды (супер-пиксель - это одна точка (x, y) на карте признаков) мы ассоциируем некоторую область на исходном изображении, и для каждой такой области назначим набор т.н. `proposals`, то есть предполагаемых обрамляющих прямоугольников, лежащих в этой области.\n",
    "\n",
    "![subnet2](images/subnet_receptive_2.jpeg) ![subnet](images/subnet_receptive.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти наборы нужны для того, чтобы связать суперпиксели активационных карт с ground-truth объектами при обучении. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchors generator\n",
    "\n",
    "В качестве основы для набора `proposals` возьмем три прямоугольника: \"горизонтальный\" (соотношение сторон H:W = 1:2, в коде - `ratio`), \"квадрат\" (H:W = 1:2) и \"вертикальный\" (H:W = 2:1).\n",
    "\n",
    "Если кроме 3 вариантов соотношения сторон взять 3 варианта для масштаба, получим набор из 9 прямоугольников. \n",
    "\n",
    "![1](images/proposals.png)\n",
    "\n",
    "\n",
    "После того, как зафиксирован набор `proposals` для одного супер-пикселя, нужно отобразить все прямоугольники (для всех супер-пикселей на всех уровнях пирамиды) на исходное изображение, чтобы связать их с ground-truth прямоугольниками объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс `AnchorsGenerator` нужен затем, чтобы сгенерировать наборы `proposals` для всех слоев, раскидать их по сетке и спроецировать на исходное изображение.\n",
    "\n",
    "**NB: Конкретные положения прямоугольников зависят как от гипер-параметров генератора, так и от размеров изображения!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinanet.anchors import generate_anchors, shift \n",
    "\n",
    "\n",
    "class AnchorsGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None, device=None):\n",
    "        \"\"\"\n",
    "        AnchorGenerator constructor.\n",
    "        \n",
    "        Args:\n",
    "            - pyramid_levels: list of log_2(image_width / pyramid_level_width) for every pyramid level.\n",
    "            - strides: list of step sizes for sliding each anchor box set for every pyramid level.\n",
    "            - sizes: list of basic sizes for anchor boxes for every pyramid level.\n",
    "            - ratios: list of anchor boxes aspect ratio sets for every pyramid level.\n",
    "            - scales: list of anchor boxes scale sets for every pyramid level.\n",
    "            - device: device to move results to.\n",
    "        \"\"\"\n",
    "        super(AnchorsGenerator, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image, verbose=False):\n",
    "        \"\"\"\n",
    "        Method for \"__call__\".\n",
    "        \n",
    "        Args:\n",
    "            - image: np.ndarray of image, shape HxWx3.\n",
    "            \n",
    "        Returns:\n",
    "            tensor of computed anchor box coordinatates, size total_num_anchors x 4.\n",
    "        \"\"\"\n",
    "        image_shape = image.shape[2:]  # h, w\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        # Compute anchors over all pyramid levels\n",
    "        \n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            \n",
    "            # Generate 9 anchors\n",
    "            anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            \n",
    "            # Grid all over the shape with stride\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{idx}\\tpyr_level = {p}\\tstride = {self.strides[idx]:3d}\\tsize={self.sizes[idx]:3d}\\tanchors shape = {shifted_anchors.shape}\")\n",
    "            \n",
    "            all_anchors = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        return torch.from_numpy(all_anchors.astype(np.float32)).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_generator = AnchorsGenerator(device=device)\n",
    "\n",
    "anchors_example = anchors_generator(tensor_example, verbose=True)[0].cpu()\n",
    "print()\n",
    "print(anchors_example.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили общее число `proposals`, каждый из которых задается четырьмя числами - x1, y1, x2, y2.\n",
    "\n",
    "Отрисуем по несколько полученных прямоугольников с каждого слоя на исходном изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_num_per_level = [61200, 15300, 3825, 1053, 315]\n",
    "anchors_per_level = np.split(anchors_example, np.cumsum(anchors_num_per_level)[:-1])\n",
    "\n",
    "num_to_show = 32\n",
    "for anchor_boxes_level in anchors_per_level:\n",
    "    image_example_with_anchors = image_example.copy()\n",
    "    for i in np.random.choice(len(anchor_boxes_level), size=num_to_show):\n",
    "        anchor_box = anchor_boxes_level[i]\n",
    "        x1, y1, x2, y2 = anchor_box.numpy().astype(np.int32)\n",
    "        cv2.rectangle(image_example_with_anchors, (x1, y1), (x2, y2), np.random.randint(255, size=3).tolist(), 2)\n",
    "    \n",
    "    show_image(image_example_with_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть модуль для генерации якорных прямоугольников, можем перейти к сеткам для классификации и регрессии на их основе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression & Classification subnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификацию (объект одного из классов или фон) и регрессию (уточнение координат объекта относительно `anchor box`) будет делать в сверточной манере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overall](images/overall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Регрессия\n",
    "\n",
    "Поскольку с каждым супер-пикселем у нас ассоциировано 9 разных `proposals`, каждый из которых задан с помощью 4 чисел, то для регрессии нам потребуется 9 x 4 = 36 чисел в каждом суперпикселе. Значит, сделаем полносверточную сеть, которая не меняет размера карт активаций (то есть нет `pooling`-слоев и `stride`=1) и имеет на выходе 36 каналов. Таким образом, в каждом из `HxW` суперпикселей выходной карты будут храниться все необходимые таргеты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** напишите код для преобразования тензора с результатами из вида `B x (num_anchors * 4) x H x W` к виду `B x total_num_anchors x 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
    "        \"\"\"\n",
    "        RegressionModel consctructor. \n",
    "        \n",
    "        RegressionModel is a fully-convolutional neural network that takes single feature pyramid level and\n",
    "        outputs tensor of size total_number_of_anchors x 4.\n",
    "        \n",
    "        Args:\n",
    "            - num_features_in: number of channels in every feature pyramid level.\n",
    "            - num_anchors: size of anchor boxes set.\n",
    "            - feature_size: size of internal activation maps.\n",
    "        \"\"\"\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run regression on single feature pyramid level.\n",
    "        \n",
    "        Args:\n",
    "            - x: feature pyramid level of shape B x num_features_in x H x W.\n",
    "            \n",
    "        Returns:\n",
    "            tensor of size B x (H * W * num_anchors) x 4.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        # out is B x C x W x H, with C = 4 * num_anchors\n",
    "        \n",
    "        # TODO: change shape of `out` tensor to be `B x total_num_anchors x 4`.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_in = 256\n",
    "regression_head = RegressionModel(num_features_in).to(device)\n",
    "\n",
    "regression_predictions = []\n",
    "for features in features_pyramid_example:\n",
    "    regression_predictions_at_level = regression_head(features)\n",
    "    regression_predictions.append(regression_predictions_at_level)\n",
    "    print(f\"input: {features.size()}\\t output: {regression_predictions_at_level.size()}\")\n",
    "regression_predictions = torch.cat(regression_predictions, dim=1) # batch x anchors x 4 [4 for bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Классификация\n",
    "\n",
    "Архитектурно голова классификации идентична голове регрессии, отличие только в размере выхода (и активации): модель возвращает столько значений для каждого бокса, сколько классов в датасете, причем с активацией `Sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n",
    "        \"\"\"\n",
    "        ClassificationModel consctructor. \n",
    "        \n",
    "        ClassificationModel is a fully-convolutional neural network that takes single feature pyramid level and\n",
    "        outputs tensor of size total_number_of_anchors x num_classes.\n",
    "        \n",
    "        Args:\n",
    "            - num_features_in: number of channels in every feature pyramid level.\n",
    "            - num_anchors: size of anchor boxes set.\n",
    "            - num_classes: number of classes in dataset (do not count the `background` class!).\n",
    "            - prior: float number to use in weights initializatin.\n",
    "            - feature_size: size of internal activation maps.\n",
    "        \"\"\"\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):        \n",
    "        \"\"\"\n",
    "        Run classification on single feature pyramid level.\n",
    "        \n",
    "        Args:\n",
    "            - x: feature pyramid level of shape B x num_features_in x H x W.\n",
    "            \n",
    "        Returns:\n",
    "            tensor of size B x (H * W * num_anchors) x num_classes.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "        # out is B x C x W x H, with C = num_classes * num_anchors\n",
    "        \n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "        out2 = out1.reshape(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.reshape(x.shape[0], -1, self.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = ClassificationModel(num_features_in, num_classes=num_classes).to(device)\n",
    "classification_results = []\n",
    "\n",
    "for feature in features_pyramid_example:\n",
    "    classification_results_at_level = classification_head(feature)\n",
    "    classification_results.append(classification_results_at_level)\n",
    "    print(f\"input: {feature.size()}\\t output: {classification_results_at_level.size()}\")\n",
    "classification_results = torch.cat(classification_results, dim=1) # batch x anchors x classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем объединить все в одну композицию, класс `RetinaNet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinanet.utils import BBoxTransform, ClipBoxes\n",
    "from retinanet.losses import FocalLoss\n",
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из интересного: метод `.forward()` работает по разному во время обучения и во время инференса:\n",
    "- Во время обучения (`self.training = True`) функция возвращает список из двух лоссов - регрессионного и классификационного.\n",
    "- Во время инференса функция возвращает предсказания: скоры, индексы классов и координаты боксов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также помимо уже разобранных модулей будут использоваться:\n",
    "- `BBoxTransform`: класс для сборки боксов из предсказаний регрессионной головы и набора якорных боксов.\n",
    "- `ClipBoxes`: класс для усечения координат боксов при выходе за границы изображения.\n",
    "- `FocalLoss`: класс для вычисления сложной функции потерь (на самом деле, в нем вычисляются оба лосса - для классификации и для регрессии).\n",
    "  Внутри него происходит, например, ассоциация GT-боксов с якорными боксами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, backbone=\"resnet50\", fpn_feature_size=256, subnets_feature_size=256, device=None):\n",
    "        super(RetinaNet, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        if backbone == \"resnet50\":\n",
    "            self.backbone = ResNet(num_classes=num_classes, block=Bottleneck, layers_sizes=(3, 4, 6, 3)).to(device)\n",
    "            self.fpn = PyramidFeatures(512, 1024, 2048, fpn_feature_size).to(device)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(backbone)\n",
    "            \n",
    "        self.regression_head = RegressionModel(num_features_in=fpn_feature_size, feature_size=subnets_feature_size).to(device)\n",
    "        self.classification_head = ClassificationModel(num_features_in=fpn_feature_size, num_classes=num_classes, feature_size=subnets_feature_size).to(self.device)\n",
    "        self._init_weights()\n",
    "        \n",
    "        self.anchors_generator = AnchorsGenerator(device=device)\n",
    "        self.transform_bboxes = BBoxTransform(device=device)\n",
    "        self.clip_bboxes = ClipBoxes()\n",
    "        self.loss_fn = FocalLoss(device=device)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply RetinaNet pipeline to inputs.\n",
    "        \n",
    "        Args:\n",
    "            - inputs: Either (images_batch, annotations) in training mode or images_batch otherwise. \n",
    "            images_batch is of shape (B x C x H x W), annotations of shape (B x N x 5) where N is a number of \n",
    "            GT boxes and 5 = 4 coords + 1 class label.\n",
    "        \n",
    "        Returns:\n",
    "            Either (classification_loss, regression_loss) in training mode or (scores, nms_class, boxes) otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            images_batch, annotations = inputs\n",
    "        else:\n",
    "            images_batch = inputs\n",
    "        \n",
    "        features = self.backbone(images_batch.to(device))\n",
    "        features_pyramid = self.fpn(features)\n",
    "        classification_results = torch.cat([self.classification_head(f) for f in features_pyramid], dim=1)\n",
    "        regression_results = torch.cat([self.regression_head(f) for f in features_pyramid], dim=1)\n",
    "        \n",
    "        anchors = self.anchors_generator(images_batch)\n",
    "        \n",
    "        if self.training:\n",
    "            return self.loss_fn(classification_results, regression_results, anchors, annotations)\n",
    "        \n",
    "        transformed_anchors = self.transform_bboxes(anchors, regression_results)\n",
    "        transformed_anchors = self.clip_bboxes(transformed_anchors, images_batch)\n",
    "\n",
    "        scores = torch.max(classification_results, dim=2, keepdim=True)[0]\n",
    "        scores_over_thresh = (scores > 0.05)[0, :, 0]\n",
    "\n",
    "        if scores_over_thresh.sum() == 0:\n",
    "            # no boxes to NMS, just return\n",
    "            return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
    "\n",
    "        # NMS iteratively removes lower scoring boxes which have an\n",
    "        # IoU greater than iou_threshold with another (higher scoring) box\n",
    "        classification_results = classification_results[:, scores_over_thresh, :]\n",
    "        transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "        scores = scores[:, scores_over_thresh, :]\n",
    "\n",
    "        anchors_nms_idx = nms(transformed_anchors[0,:,:], scores[0,:,0], 0.5)\n",
    "\n",
    "        nms_scores, nms_class = classification_results[0, anchors_nms_idx, :].max(dim=1)\n",
    "\n",
    "        return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        prior = 0.01\n",
    "\n",
    "        self.classification_head.output.weight.data.fill_(0)\n",
    "        self.classification_head.output.bias.data.fill_(-np.log((1.0 - prior) / prior))\n",
    "\n",
    "        self.regression_head.output.weight.data.fill_(0)\n",
    "        self.regression_head.output.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим веса из оригинального репозитория (модифицированные для нашего класса) и сделаем пробный прогон на тестовом изображении. Веса получены обучением модели на датасете COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet = RetinaNet(num_classes=80, device=device)\n",
    "retinanet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./FIXED_coco_resnet_50_map_0_335_state_dict.pt\", \"rb\") as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "retinanet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем на результат работы модели на тестовом примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_show(model, image_or_tensor, threshold, class_to_label_map, verbose=False):\n",
    "    if isinstance(image_or_tensor, np.ndarray):\n",
    "        tensor = make_tensor(image_or_tensor)\n",
    "        image = image_or_tensor\n",
    "    elif isinstance(image_or_tensor, torch.Tensor):\n",
    "        image = (image_or_tensor.numpy()[0] * 0.25) + 0.5\n",
    "        tensor = image_or_tensor.permute(0, 3, 1, 2)\n",
    "    else:\n",
    "        raise NotImplementedError(type(image_or_tensor))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        nms_scores, nms_classes, bboxes = model(tensor)\n",
    "    \n",
    "    nms_scores = nms_scores.cpu()\n",
    "    nms_classes = nms_classes.cpu()\n",
    "    bboxes = bboxes.cpu()\n",
    "    \n",
    "    if verbose:\n",
    "        print(nms_scores.size(), nms_scores[:8])\n",
    "        print(nms_classes.size(), nms_classes[:8])\n",
    "        print(bboxes.size(), bboxes[:8])\n",
    "        \n",
    "    image_with_predictions = draw_predictions(image, bboxes, nms_scores, nms_classes, class_to_label_map, threshold=threshold)\n",
    "    show_image(image_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./coco_id_to_name.json\", \"rt\") as fp:\n",
    "    coco_class_to_label_map = json.load(fp)\n",
    "coco_class_to_label_map = {int(k) - 1: v for k, v in coco_class_to_label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_show(retinanet, image_example, 0.5, coco_class_to_label_map, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Дообучение RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, убедимся в том, что наша модель может обучаться. \n",
    "\n",
    "У нас есть кусок датасета [OpenImages](https://storage.googleapis.com/openimages/web/index.html), которая содержит примеры класса `Ball`.\n",
    "Будем обучать нашу модель детектировать 1 класс - мячи, шары и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Подгрузка данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import DetectionDataset\n",
    "\n",
    "from retinanet.dataloader import Normalizer, Resizer, Augmenter, collater, AspectRatioBasedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    Normalizer(), Augmenter(), Resizer()\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    Normalizer(), Resizer()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к созданию датасетов и даталоадеров.\n",
    "\n",
    "Обратите внимание на `AspectRatioBasedSampler`: \n",
    "* Для обучения моделей в `pytorch` необходимо, чтобы все элементы в батче были одного размера.\n",
    "* Поскольку мы учим полносверточную архитектуру на полных изображениях (а не на кропах, как в классификации), то нам нужно отресайзить их к одному размеру. \n",
    "Если у изображений разное соотношение сторон, мы будем добавлять рамки (padding).\n",
    "* `AspectRatioBasedSampler` группирует изображения в датасете таким образом, чтобы в батч попадали изображения с как можно более похожим соотношением сторон, чтобы не делать слишком большие паддинги.\n",
    "\n",
    "Датасеты устроены так, чтобы возвращать тензор изображения и тензор аннотаций к нему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DetectionDataset(data_dict_file=\"./oi5_ball_filename_to_bbox_train.json\", transforms=train_transforms)\n",
    "train_sampler = AspectRatioBasedSampler(train_dataset, batch_size=2, drop_last=False) # deepdive\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=8, collate_fn=collater, batch_sampler=train_sampler)\n",
    "\n",
    "val_dataset = DetectionDataset(data_dict_file=\"./oi5_ball_filename_to_bbox_val.json\", transforms=val_transforms)\n",
    "val_sampler = AspectRatioBasedSampler(val_dataset, batch_size=2, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=8, collate_fn=collater, batch_sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_dataset[0]\n",
    "print(item[\"img\"].size())    # H x W x 3\n",
    "print(item[\"annot\"].size())  # N x 5; 4 coords + 1 label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь непосредственно к модели.\n",
    "\n",
    "Во-первых, нужно заменить головы для классификации и регрессии для дообучения.\n",
    "При этом у классификационной головы нужно указать число классов - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet.classification_head = ClassificationModel(num_features_in, num_classes=1)\n",
    "retinanet.regression_head = RegressionModel(num_features_in)\n",
    "retinanet.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, потребуется функция для переключения модулей нашей сети в режим обучения; сделаем так, чтобы можно было учить отдельные модули.\n",
    "\n",
    "**Вопрос**: зачем \"замораживать\" BN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_training(model, backbone=False, fpn=False, regression_head=True, classification_head=True, freeze_bn=False):\n",
    "    model.train()\n",
    "    model.requires_grad_(True)\n",
    "    \n",
    "    if not backbone:\n",
    "        model.backbone.eval()\n",
    "        model.backbone.requires_grad_(False)\n",
    "    \n",
    "    elif freeze_bn:\n",
    "        for layer in model.backbone.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "    \n",
    "    if not fpn:\n",
    "        model.fpn.eval()\n",
    "        model.fpn.requires_grad_(False)\n",
    "    \n",
    "    if not regression_head:\n",
    "        model.regression_head.eval()\n",
    "        model.regression_head.requires_grad_(False)\n",
    "\n",
    "    if not classification_head:\n",
    "        model.classification_head.eval()\n",
    "        model.classification_head.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И простая функция для обучения на 1 эпохе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def routine(retina_model, scheduler):\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for iter_num, data in tqdm.tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        classification_loss, regression_loss = retina_model([data['img'].to(device).float(), data['annot'].to(device)])\n",
    "        classification_loss = classification_loss.mean()\n",
    "        regression_loss = regression_loss.mean()\n",
    "\n",
    "        loss = classification_loss + regression_loss\n",
    "\n",
    "        if bool(loss == 0):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(retina_model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(float(loss))\n",
    "        if iter_num + 1 ==len(train_dataloader):\n",
    "            print(\n",
    "                'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Epoch loss: {:1.5f}'.format(\n",
    "                    epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(epoch_loss)))\n",
    "        \n",
    "    scheduler.step(np.mean(epoch_loss)) # should be val loss\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала обучим только новые головы в течение 1 эпохи:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set_to_training(retinanet, backbone=False, fpn=False, regression_head=True, classification_head=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "params = [p for p in retinanet.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "epochs = 1\n",
    "for epoch_num in range(epochs):\n",
    "    data = routine(retinanet, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И потом обучим всю сеть (с \"замороженными\" BN) - уже подольше."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set_to_training(retinanet, backbone=True, fpn=True, regression_head=True, classification_head=True, freeze_bn=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "params = [p for p in retinanet.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "epochs = 10\n",
    "for epoch_num in range(epochs):\n",
    "    routine(retinanet, scheduler)\n",
    "    \n",
    "    if epoch_num % 10 == 0 or epoch_num + 1 == epochs:\n",
    "        torch.save(retinanet.state_dict(), f\"dev_balls_retinanet_{epoch_num}.pt\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для иллюстрации результатов выберем случайные изображения из выборки и отрисуем предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"oi5_ball_retinanet_ep=10.pt\", \"rb\") as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "retinanet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    index = np.random.randint(len(train_dataset))\n",
    "    tensor = train_dataset[index][\"img\"].unsqueeze(0)\n",
    "    run_and_show(retinanet, tensor, 0.5, {0: \"ball\"}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    index = np.random.randint(len(val_dataset))\n",
    "    tensor = val_dataset[index][\"img\"].unsqueeze(0)\n",
    "    run_and_show(retinanet, tensor, 0.5, {0: \"ball\"}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Резюме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы мы учили детектор для какой-то конкретной задачи, то:\n",
    "\n",
    "1. Сделали бы грамотную валидацию (с подсчетом метрик типа `mAP`)\n",
    "2. Учились бы на нескольких `GPU` (для адекватного размера батча)\n",
    "3. Более аккуратно подбирали якорные боксы\n",
    "4. По возможности использовали библиотечные модели (а не кустарные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
